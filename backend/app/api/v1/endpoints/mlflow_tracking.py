"""MLflow tracking endpoints for AI tutor monitoring and analytics."""

from fastapi import APIRouter, Depends, HTTPException, status
from typing import List, Optional, Dict, Any
from pydantic import BaseModel
from datetime import datetime

from app.api.v1.deps import CurrentUser
from app.core.mlflow_config import mlflow_tracker


router = APIRouter()


# =========================================================================
# Request/Response Models
# =========================================================================

class GraphNode(BaseModel):
    """Knowledge graph node"""
    node: str
    relation: Optional[str] = None
    next_node: Optional[str] = None
    similarity_score: Optional[float] = None


class GraphExplorationRequest(BaseModel):
    """GraphRAG path tracing request"""
    question_id: str
    wrong_concept: str
    graph_path: List[Dict[str, Any]]
    retrieval_params: Dict[str, Any]  # {"depth": 3, "similarity_threshold": 0.7}
    final_explanation: str


class PromptComparisonRequest(BaseModel):
    """Prompt experimentation request"""
    question_id: str
    user_answer: str
    correct_answer: str
    prompt_variants: Dict[str, str]  # {"v1_strict": "...", "v2_friendly": "..."}
    generated_responses: Dict[str, str]


class UserFeedbackRequest(BaseModel):
    """User feedback logging request"""
    session_id: str
    question_id: str
    ai_explanation: str
    feedback_type: str  # "thumbs_up", "thumbs_down", "followup_question"
    followup_text: Optional[str] = None
    understanding_score: Optional[int] = None  # 1-5


class LLMCostRequest(BaseModel):
    """LLM cost tracking request"""
    task_type: str  # "simple_greeting", "concept_explanation", "complex_reasoning"
    model_name: str  # "gpt-4o", "gpt-4o-mini"
    input_tokens: int
    output_tokens: int
    estimated_cost: float
    latency_ms: float
    response_quality: Optional[str] = None


class TrackingResponse(BaseModel):
    """Generic tracking response"""
    run_id: str
    message: str


class AnalyticsResponse(BaseModel):
    """Analytics report response"""
    data: Any
    generated_at: datetime


# =========================================================================
# Scenario A: GraphRAG Path Tracing
# =========================================================================

@router.post("/trace-graph-exploration", response_model=TrackingResponse)
async def trace_graph_exploration(
    request: GraphExplorationRequest,
    current_user: CurrentUser
):
    """
    GraphRAGì˜ ì§€ì‹ ê·¸ë˜í”„ íƒìƒ‰ ê²½ë¡œë¥¼ ì¶”ì í•©ë‹ˆë‹¤.

    **ì‹œë‚˜ë¦¬ì˜¤ A**: ê°œë°œìê°€ "ì™œ AI íŠœí„°ê°€ 'ì •ê·œí™”'ì—ì„œ 'SQL ë¬¸ë²•'ìœ¼ë¡œ ê°‘ìê¸° íŠ€ì—ˆëŠ”ì§€"
    íƒìƒ‰ ê²½ë¡œë¥¼ ì‹œê°í™”í•˜ì—¬ ë””ë²„ê¹…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """
    try:
        run_id = mlflow_tracker.trace_graph_exploration(
            user_id=current_user.clerk_id,
            question_id=request.question_id,
            wrong_concept=request.wrong_concept,
            graph_path=request.graph_path,
            retrieval_params=request.retrieval_params,
            final_explanation=request.final_explanation
        )

        return TrackingResponse(
            run_id=run_id,
            message=f"Graph exploration path tracked successfully. View at MLflow UI."
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to track graph exploration: {str(e)}"
        )


# =========================================================================
# Scenario B: Prompt Experimentation
# =========================================================================

@router.post("/compare-prompts", response_model=TrackingResponse)
async def compare_prompts(
    request: PromptComparisonRequest,
    current_user: CurrentUser
):
    """
    ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ ë²„ì „ì„ ë¹„êµ ì‹¤í—˜í•©ë‹ˆë‹¤.

    **ì‹œë‚˜ë¦¬ì˜¤ B**: "ì—„ê²©í•œ ì„ ìƒë‹˜ í†¤ vs ì¹œì ˆí•œ ì½”ì¹˜ í†¤" ì¤‘ ì–´ëŠ ê²ƒì´
    ìˆ˜í—˜ìƒì˜ ë©˜íƒˆì„ ì¼€ì–´í•˜ë©´ì„œë„ í•™ìŠµ íš¨ê³¼ê°€ ë†’ì€ì§€ ë¹„êµí•©ë‹ˆë‹¤.
    """
    try:
        run_ids = mlflow_tracker.compare_prompts(
            question_id=request.question_id,
            user_answer=request.user_answer,
            correct_answer=request.correct_answer,
            prompt_variants=request.prompt_variants,
            generated_responses=request.generated_responses
        )

        return TrackingResponse(
            run_id=",".join(run_ids),
            message=f"Prompt comparison tracked. {len(run_ids)} variants logged."
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to compare prompts: {str(e)}"
        )


# =========================================================================
# Scenario C: User Feedback Loop
# =========================================================================

@router.post("/log-user-feedback", response_model=TrackingResponse)
async def log_user_feedback(
    request: UserFeedbackRequest,
    current_user: CurrentUser
):
    """
    ì‚¬ìš©ìì˜ AI íŠœí„° í”¼ë“œë°±ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    **ì‹œë‚˜ë¦¬ì˜¤ C**: í•™ìƒì´ "ì´í•´ ì•ˆ ë¼ìš”(ğŸ‘)" ë²„íŠ¼ì„ ëˆ„ë¥´ë©´,
    í•´ë‹¹ ì„¸ì…˜ì„ 'review_needed' íƒœê·¸ë¡œ ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì—
    "í•™ìƒë“¤ì´ ê°€ì¥ ì´í•´ ëª» í•œ ê°œë… TOP 5"ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """
    try:
        run_id = mlflow_tracker.log_user_feedback(
            session_id=request.session_id,
            user_id=current_user.clerk_id,
            question_id=request.question_id,
            ai_explanation=request.ai_explanation,
            user_feedback=request.feedback_type,
            followup_text=request.followup_text,
            understanding_score=request.understanding_score
        )

        return TrackingResponse(
            run_id=run_id,
            message="User feedback logged successfully."
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to log user feedback: {str(e)}"
        )


# =========================================================================
# Scenario D: Cost Monitoring
# =========================================================================

@router.post("/log-llm-cost", response_model=TrackingResponse)
async def log_llm_cost(
    request: LLMCostRequest,
    current_user: CurrentUser
):
    """
    LLM í˜¸ì¶œ ë¹„ìš© ë° ì„±ëŠ¥ì„ ì¶”ì í•©ë‹ˆë‹¤.

    **ì‹œë‚˜ë¦¬ì˜¤ D**: "ë‹¨ìˆœí•œ ì¸ì‚¬ë§ì—ëŠ” GPT-4o-mini,
    ë³µì¡í•œ ê°œë… ì¶”ë¡ ì—ëŠ” GPT-4o"ê°€ ì œëŒ€ë¡œ ë¼ìš°íŒ…ë˜ëŠ”ì§€ ê²€ì¦í•˜ê³ ,
    ì‚¬ìš©ì 1ëª…ë‹¹ í‰ê·  ë¹„ìš©ì„ ì‚°ì¶œí•©ë‹ˆë‹¤.
    """
    try:
        run_id = mlflow_tracker.log_llm_call(
            user_id=current_user.clerk_id,
            task_type=request.task_type,
            model_name=request.model_name,
            input_tokens=request.input_tokens,
            output_tokens=request.output_tokens,
            estimated_cost=request.estimated_cost,
            latency_ms=request.latency_ms,
            response_quality=request.response_quality
        )

        return TrackingResponse(
            run_id=run_id,
            message="LLM cost logged successfully."
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to log LLM cost: {str(e)}"
        )


# =========================================================================
# Analytics & Reporting
# =========================================================================

@router.get("/analytics/top-failed-concepts", response_model=AnalyticsResponse)
async def get_top_failed_concepts(
    current_user: CurrentUser,
    limit: int = 5
):
    """
    í•™ìƒë“¤ì´ ê°€ì¥ ì´í•´í•˜ê¸° ì–´ë ¤ì›Œí•˜ëŠ” ê°œë… TOP Nì„ ë°˜í™˜í•©ë‹ˆë‹¤.

    **í™œìš©**: GraphRAG ì§€ì‹ ê·¸ë˜í”„ì— "ë” ì‰¬ìš´ ì„¤ëª… ë…¸ë“œ"ë¥¼ ì¶”ê°€í• 
    ìš°ì„ ìˆœìœ„ë¥¼ ê²°ì •í•˜ëŠ” ë°ì´í„° ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    try:
        top_concepts = mlflow_tracker.get_top_failed_concepts(limit=limit)

        return AnalyticsResponse(
            data=top_concepts,
            generated_at=datetime.utcnow()
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get analytics: {str(e)}"
        )


@router.get("/analytics/average-cost", response_model=AnalyticsResponse)
async def get_average_cost_per_user(
    current_user: CurrentUser,
    user_id: Optional[str] = None
):
    """
    ì‚¬ìš©ì 1ëª…ë‹¹ í‰ê·  LLM ë¹„ìš©ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

    **í™œìš©**: ì›” êµ¬ë…ë£Œ ì±…ì •(â‚©10,000)ì´ LLM ë¹„ìš©ì„ ì»¤ë²„í•  ìˆ˜ ìˆëŠ”ì§€,
    ì„œë²„ ìš´ì˜ ì˜ˆì‚°ì„ ì–´ë–»ê²Œ ìˆ˜ë¦½í•´ì•¼ í•˜ëŠ”ì§€ ì •í™•í•œ ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    try:
        avg_cost = mlflow_tracker.get_average_cost_per_user(user_id=user_id)

        return AnalyticsResponse(
            data={
                "average_cost_usd": avg_cost,
                "average_cost_krw": avg_cost * 1300,  # Approximate USD to KRW
                "user_id": user_id or "all_users"
            },
            generated_at=datetime.utcnow()
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to calculate average cost: {str(e)}"
        )
